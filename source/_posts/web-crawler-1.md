---
title: web crawler-1
date: 2020-02-13 13:36:38
tags: 搜索引擎
categories: [软件,WEB]
---

### 网络爬虫的基本原理

**简述:**
* 英文:web crawler/spider
* 用来自动浏览万维网的网络机器人
* 其目的一般为编纂网络索引
* 搜索引擎抓取的重要组成部分
* 爬虫的主要目的是将互联网上的网页下载到本地形成一个或联网内容的镜像内分
* 网络搜索引擎等站点通过爬虫软件更新自身的网站内容或其对其他网站的索引
* 网络爬虫可以将自己所访问的页面保存下来，以便搜索引擎事后生成索引供用户搜索
* 同一时间点的执行逻辑用同一语句描述

**网络爬虫的基本结构及工作流程:**

* 基本工作流程简述:
1. 首选选取一部分经过挑选的种子URL
2. 将这些种子URL放入待抓取URL队列
3. 从待抓取URL队列中取出待抓取URL，解析其URL，并得到其主机的IP，并将URL对应的网页下载下来，存储到已下载的网页库中
4. 同时，将这些URL放进已抓取URL队列
4. 分析已抓取URL队列中的URL，分析其中的其他URL，并且将URL放入待抓取URL队列，从而进入下一个循环

* 逻辑简述:
> 种子URL->待抓取URL->[已下载网页库内[读取URL，DNS解析，网页下载]]/[[已抓取URL内[将已下载URL放进已抓取URL队列]]-抽取新的URL放入待抓取URL队列->待抓取URL]
> /符号代表同级关系，执行步骤相同

---

* 从爬虫的角度对互联网进行划分
* 爬虫可以将互联网页面大致划分为五个部分:
1. 已下载未过期网页
2. 已下载已过期网页: 抓取到的网页实际上是互联网内容的一个镜像与备份，互联网是动态变化的，一部分互联网上的内容已经发生了变化，这时，这部分抓取到的网页就已经过期了
3. 待下载->若干个可知网页: 也就是待抓取URL队列中的那些页面
4. 可知网页: 还没有抓取下来，也没有在待抓取URL队列中，但是可以通过对已抓取页面或者待抓取URL对应页面进行分析获取到的URL，认为是可知网页
5.不可知网页: 还有一部分网页，爬虫是无法直接抓取下载的，称为不可知网页

---

### 抓取策略:
* 待抓取URL队列是构成爬虫系统中必不可失的一部分
* 待抓取URL队列中的URL以什么样的顺序排序，将会涉及到所选抓取页面的优先级问题
* 而决定这些URL排列顺序的方法，就被称为抓取策略

---

**几种常见的抓取策略:**

**深度优先遍历策略**
1. 深度优先遍历策略是指网络爬虫会从起始页开始，一个链接接一个链接的跟踪下去
2. 处理完这条线路之后再转入下一个起始页，继续跟踪链接

**逻辑简述:**
> 起始页A，不同路线的链接页面B到I，链接关联:(`F->G，E->H->I，B，C，D`)
1. `A->F->G`
2. `A->E->H->I`
3. `A->B`
4. `A->C`
5. `A->D`

---

**宽度优先遍历策略**

**基本思路:**
1. 将新下载网页中发现的链接直接插入待抓取URL队列的末尾
2. 也就是指网络爬虫会抓取起始网页中链接的所有网页
3. 然后再选择其中的一个链接网页，继续抓取再次网页中链接的所有网页

**逻辑简述:**
> 起始页A，不同路线的链接页面B到I，(`F->G，E->H->I，B，C，D`)
> `A->B->C->D->E->F，G，H，I`

---

**反向链接数策略**
1. 反向链接数是指一个网页被其他网页链接指向的数量
2. 反向链接数表示的是一个网页的内容受到其他人的推荐的程度
3. 因此，很多时候搜索引擎的抓取系统会使用这个指标来评价网页的重要程度，从而决定不同网页的抓取先后顺序
4. 在真实的网络环境中，由于广告链接、作弊链接的存在，反向链接数不能完全等同于所选链接重要程度
5. 所以搜索引擎往往考虑一些可靠的反向链接数

---

**Partial PageRank策略**
1. Partial PageRank算法借鉴了PageRank算法的思想：
2. 对于已经下载的网页，连同待抓取URL队列中的URL，形成网页集合，计算每个页面的PageRank值
3. 在计算完之后，将待抓取URL队列中的URL按照PageRank值的大小排列
4. 最后按照该顺序抓取页面

* 每次抓取一个页面，就会重新计算PageRank值
* 一种折中方案是：每抓取K个页面后，重新计算一次PageRank值
* 但是这种情况还会衍生出另一个问题:对于已经下载下来的页面中分析出的链接，也就是之前提到的未知网页那一部分，暂时是没有PageRank值的
* 为了解决这个问题，会给这些页面一个临时的PageRank值:将这个网页所有入链传递进来的PageRank值进行汇总，这样就形成了该未知页面的PageRank值，从而参与排序

---

**OPIC策略策略**
* 该算法实际上也是对页面进行一个重要性打分

**基本思路:**
1. 在算法开始前，给所有页面一个相同的初始现金(cash)
2. 当下载了某个页面P之后，将P的现金分摊给所有从P中分析出的链接，并且将P的现金清空
3. 对于待抓取URL队列中的所有页面按照现金数进行排序

---

**大站优先策略**

**基本思路:**
1. 对于待抓取URL队列中的所有网页，根据所属的网站进行分类
2. 对于待下载页面数多的网站，优先下载
3. 这个策略也因此叫做大站优先策略

---

### 更新策略

**简述:**
* 互联网是实时变化的，具有很强的动态性
* 网页更新策略主要是决定何时更新之前已经下载过的页面


**常见的更新策略有以下三种:**

**历史参考策略**

**基本思路:**
1. 根据页面以往的历史更新数据，预测该页面未来何时会发生变化
2. 一般来说，是通过泊松过程进行建模进行预测

---

**用户体验策略**

* 索引擎针对于某个查询条件能够返回数量巨大的结果
* 但用户往往只关注前几页结果
* 所以抓取系统可以优先更新那些现实在查询结果前几页中的网页，而后再更新那些后面的网页
* 这种更新策略也是需要用到历史信息的

**基本思路:**
1. 用户体验策略保留网页的多个历史版本
2. 并且根据过去每次内容变化对搜索质量的影响
3. 然后得出一个平均值，用这个值作为决定何时重新抓取的依据

---

**聚类抽样策略**
* 前面提到的两种更新策略都有一个前提:需要网页的历史信息
* 这样就存在两个问题:
1. 系统要是为每个系统保存多个版本的历史信息，无疑增加了很多的系统负担
2. 要是新的网页完全没有历史信息，就无法确定更新策略

**基本思路:**
* 这种策略认为，网页具有很多属性，类似属性的网页，可以认为其更新频率也是类似的
* 要计算某一个类别网页的更新频率，只需要对这一类网页抽样，以他们的更新周期作为整个类别的更新周期

**逻辑简述:**
> 原始网页-网页聚类->[聚类一/聚类二/聚类三]->对不同类聚类分别抽样->分别确定更新周期

---

### 分布式抓取结构

**概述:**
* 通常的抓取系统需要面对的是整个互联网上数以亿计的网页
* 因此单个抓取程序不可能完成这样的任务，所以往往需要多个抓取程序一起来处理
* 一般来说抓取系统往往是一个分布式的三层结构:1.网页，2.数据中心，3.网络爬虫

**逻辑简述:**
> `[原始网页<-网络爬虫->[数据中心(远端服务器)]]<-[所有的数据中心(Global)]`
1. 不同的数据中心分布在不同的地理位置
2. 在每个数据中心内有若干台抓取服务器
3. 而每台抓取服务器上可能部署了若干套爬虫程序
4. 这就构成了一个基本的分布式抓取系统

---

* 对于一个数据中心内的不同抓取服务器，有不同的协同工作方式
* 协同工作的方式有以下几种:
1. 主从式(Master-Slave)

**逻辑(同一时间点)简述:**
> `[待抓取URL队列(若干数量)]->[(远端服务器)Master->Slave1/Slave2/Slave3]<-下载-原始网页`

1. 对于主从式而言，有一台专门的Master服务器来维护待抓取URL队列
2. 它负责每次将URL分发到不同的Slave服务器，而Slave服务器则负责实际的网页下载工作
3. Master服务器除了维护待抓取URL队列以及分发URL之外，还要负责调解各个Slave服务器的负载情况
4. 以免某些Slave服务器过于清闲或者劳累
5. 这种模式下，Master往往容易成为系统瓶颈

---

2. 对等式(Peer to Peer)
> `[待抓取URL队列(若干数量)]->Hash->[(抓取服务器)server0/server1/server2]<-原始网页`

* 在这种模式下，所有的抓取服务器在分工上没有不同

1. 每一台抓取服务器都可以从待抓取在URL队列中获取URL
2. 然后对该URL的主域名的hash值H
3. 再计算H mod m(其中m是服务器的数量，此时设m为3)
4. 最后计算得到的数就是处理该URL的主机编号

**举例:**
* 有server0，server1，server2分别为0，1，2号服务器
* 假设对于URL-`www.baidu.com`，计算器`hash`值`H=8`，`m=3`，则`H mod m=2`，因此由编号为2的服务器进行该链接的抓取
* 假设这时候是0号服务器拿到这个URL，那么它将该URL转给服务器2，由服务器2进行抓取

* 这种模式有一个问题，当有一台服务器死机或者添加新的服务器，那么所有URL的哈希求余的结果就都要变化
* 也就是说，这种方式的扩展性不佳
* 针对这种情况，又有一种改进方案被提出来
* 这种改进的方案是一致性哈希法来确定服务器分工
* 其基本结构流程:
1.  一致性哈希将URL的主域名进行哈希运算，映射为一个范围在`0-2^32`之间的某个数
2. 而将这个范围平均的分配给m台服务器，根据URL主域名哈希运算的值所处的范围判断是哪台服务器来进行抓取
3. 如果某一台服务器出现问题，那么本该由该服务器负责的网页则按照顺时针顺延，由下一台服务器进行抓取
4. 如此一来，即便某台服务器出现问题，也不会影响其他服务器的工作

**逻辑简述:**
> `URL->hash->1,2,3,4<-hash<-URL`

---

**相关概念:**
* 搜索引擎
* 网络爬虫
* 爬虫策略/抓取策略
* 爬虫，索引，检索，排序


